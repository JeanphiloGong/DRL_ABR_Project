### **ğŸš€ è®­ç»ƒ DRL æ¨¡å‹æ­¥éª¤**
ä½ å·²ç»æˆåŠŸæ”¶é›†äº†æ•°æ®ï¼Œæ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨ **æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰** æ¥è®­ç»ƒä¸€ä¸ª **æ™ºèƒ½ç ç‡æ§åˆ¶ï¼ˆABRï¼‰ç­–ç•¥**ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ **PyTorch** ç»“åˆ **DQNï¼ˆDeep Q-Networkï¼‰** è¿›è¡Œè®­ç»ƒã€‚

---

## **âœ… 1. å®šä¹‰ DRL ä»»åŠ¡**
### **ğŸ¯ çŠ¶æ€ç©ºé—´ï¼ˆState Spaceï¼‰**
ä½ çš„æ•°æ®é›†åŒ…å«ï¼š
- **ç½‘ç»œæŒ‡æ ‡**
  - `throughput`ï¼ˆç½‘ç»œå¸¦å®½ä¼°è®¡ï¼‰
  - `latency`ï¼ˆç½‘ç»œæ—¶å»¶ï¼‰
- **å®¢æˆ·ç«¯æŒ‡æ ‡**
  - `buffer`ï¼ˆç¼“å†²åŒºå¤§å°ï¼‰
  - `bitrate`ï¼ˆå½“å‰ç ç‡ï¼‰
  - `switch_count`ï¼ˆç ç‡åˆ‡æ¢æ¬¡æ•°ï¼‰

ğŸ“Œ **å®šä¹‰çŠ¶æ€å‘é‡**ï¼š
\[
S_t = (bitrate, buffer, throughput, latency, switch\_count)
\]
---

### **ğŸ¯ åŠ¨ä½œç©ºé—´ï¼ˆAction Spaceï¼‰**
- é€‰æ‹©ä¸åŒçš„ç ç‡ï¼š
  - `240p (500kbps)`
  - `360p (1000kbps)`
  - `720p (2500kbps)`
  - `1080p (5000kbps)`
  - `4K (8000kbps)`

ğŸ“Œ **å®šä¹‰åŠ¨ä½œé›†åˆ**ï¼š
\[
A = \{500, 1000, 2500, 5000, 8000\}
\]

---

### **ğŸ¯ å¥–åŠ±å‡½æ•°ï¼ˆReward Functionï¼‰**
ä¸€èˆ¬é‡‡ç”¨ï¼š
\[
R = w_1 \times bitrate - w_2 \times rebuffering - w_3 \times quality\_switches
\]
- **æ¸…æ™°åº¦å¥–åŠ±**ï¼šæ›´é«˜çš„ `bitrate` å¾—åˆ°æ›´é«˜çš„å¥–åŠ±ï¼ˆ`w_1`ï¼‰
- **å¡é¡¿æƒ©ç½š**ï¼šå¦‚æœ `buffer` å¤ªä½ï¼Œæƒ©ç½š `rebuffering`ï¼ˆ`w_2`ï¼‰
- **ç ç‡åˆ‡æ¢æƒ©ç½š**ï¼šé¿å…é¢‘ç¹åˆ‡æ¢ï¼ˆ`w_3`ï¼‰

ğŸ“Œ **å…·ä½“å®ç°**
```python
def compute_reward(bitrate, rebuffering, switch_count, w1=1.0, w2=3.0, w3=1.5):
    return w1 * bitrate - w2 * rebuffering - w3 * switch_count
```

---

## **âœ… 2. é¢„å¤„ç†æ•°æ®**
å…ˆè¯»å–æ•°æ®å¹¶è½¬æ¢æˆ DRL è®­ç»ƒæ ¼å¼ã€‚

### **ğŸ“Œ è¯»å– SQLite æ•°æ®**
```python
import sqlite3
import pandas as pd

DATABASE = 'dash_metrics.db'

# è¯»å–æ•°æ®
conn = sqlite3.connect(DATABASE)
df = pd.read_sql_query("SELECT * FROM dash_metrics ORDER BY timestamp", conn)
conn.close()

print(df.head())
```

### **ğŸ“Œ é¢„å¤„ç†æ•°æ®**
```python
import numpy as np

# å½’ä¸€åŒ–æ•°æ®
def normalize(x, min_val, max_val):
    return (x - min_val) / (max_val - min_val)

df['buffer'] = normalize(df['buffer'], 0, df['buffer'].max())
df['throughput'] = normalize(df['throughput'], 0, df['throughput'].max())
df['latency'] = normalize(df['latency'], 0, df['latency'].max())

# è®¡ç®—å¥–åŠ±
df['reward'] = df.apply(lambda row: compute_reward(row['bitrate'], 1/row['buffer'] if row['buffer'] > 0 else 10, row['switch_count']), axis=1)

# çŠ¶æ€-åŠ¨ä½œå¯¹
states = df[['bitrate', 'buffer', 'throughput', 'latency', 'switch_count']].values
rewards = df['reward'].values
```

---

## **âœ… 3. è®­ç»ƒ DRL æ¨¡å‹ï¼ˆDQNï¼‰**
æˆ‘ä»¬ä½¿ç”¨ **Deep Q-Networkï¼ˆDQNï¼‰** æ¥è®­ç»ƒ ABR ç­–ç•¥ã€‚

### **ğŸ“Œ 1. å®šä¹‰ Q ç½‘ç»œ**
```python
import torch
import torch.nn as nn
import torch.optim as optim
import random

# Q-Network ç»“æ„
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)  # è¾“å‡º Q å€¼
```

---

### **ğŸ“Œ 2. è®­ç»ƒ DQN**
```python
# å®šä¹‰å‚æ•°
STATE_SIZE = states.shape[1]
ACTION_SIZE = 5  # 5 ç§ç ç‡é€‰æ‹©
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON = 1.0  # åˆå§‹æ¢ç´¢ç‡
EPSILON_DECAY = 0.995
EPSILON_MIN = 0.01
BATCH_SIZE = 32
MEMORY_SIZE = 1000

# åˆå§‹åŒ–ç½‘ç»œ
q_network = QNetwork(STATE_SIZE, ACTION_SIZE)
optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)
criterion = nn.MSELoss()

# ç»éªŒå›æ”¾
memory = []

# è®­ç»ƒå¾ªç¯
for episode in range(500):  # è®­ç»ƒ 500 è½®
    state = states[random.randint(0, len(states) - 1)]  # éšæœºåˆå§‹åŒ–çŠ¶æ€
    total_reward = 0
    
    for step in range(100):  # æ¯è½®æœ€å¤š 100 æ­¥
        # é€‰æ‹©åŠ¨ä½œ (Epsilon-Greedy)
        if random.random() < EPSILON:
            action = random.randint(0, ACTION_SIZE - 1)  # éšæœºåŠ¨ä½œ
        else:
            with torch.no_grad():
                action = torch.argmax(q_network(torch.tensor(state, dtype=torch.float32))).item()

        # è®¡ç®—å¥–åŠ±
        reward = rewards[action]
        next_state = states[random.randint(0, len(states) - 1)]  # éšæœºé€‰æ‹©ä¸‹ä¸€çŠ¶æ€

        # å­˜å…¥ç»éªŒå›æ”¾æ± 
        memory.append((state, action, reward, next_state))
        if len(memory) > MEMORY_SIZE:
            memory.pop(0)

        # ç»éªŒå›æ”¾è®­ç»ƒ
        if len(memory) > BATCH_SIZE:
            batch = random.sample(memory, BATCH_SIZE)
            state_batch = torch.tensor([b[0] for b in batch], dtype=torch.float32)
            action_batch = torch.tensor([b[1] for b in batch], dtype=torch.int64)
            reward_batch = torch.tensor([b[2] for b in batch], dtype=torch.float32)
            next_state_batch = torch.tensor([b[3] for b in batch], dtype=torch.float32)

            # è®¡ç®— Q å€¼
            q_values = q_network(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)
            with torch.no_grad():
                target_q_values = reward_batch + GAMMA * q_network(next_state_batch).max(dim=1)[0]

            loss = criterion(q_values, target_q_values)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        total_reward += reward
        state = next_state

    EPSILON = max(EPSILON * EPSILON_DECAY, EPSILON_MIN)
    print(f"Episode {episode}, Total Reward: {total_reward}, Epsilon: {EPSILON:.3f}")
```

---

## **âœ… 4. è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æ¨¡å‹**
```python
torch.save(q_network.state_dict(), "abr_q_network.pth")
```

---

## **ğŸš€ æ€»ç»“**
| **æ­¥éª¤** | **ä»»åŠ¡** |
|----------|---------|
| **1. æ•°æ®é¢„å¤„ç†** | è¯»å– SQLite æ•°æ®ï¼Œå½’ä¸€åŒ– |
| **2. å®šä¹‰çŠ¶æ€ç©ºé—´** | `bitrate, buffer, throughput, latency, switch_count` |
| **3. å®šä¹‰å¥–åŠ±å‡½æ•°** | `bitrate - rebuffering - switch_count` |
| **4. è®­ç»ƒ DQN** | ç»éªŒå›æ”¾ + Q ç½‘ç»œæ›´æ–° |
| **5. è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹** | `torch.save()` |

---

## **ğŸš€ ä¸‹ä¸€æ­¥**
- ç”¨ **DRL è®­ç»ƒå‡ºçš„æ¨¡å‹** è¿›è¡Œ **åœ¨çº¿æ¨ç†**ï¼Œè®© DASH.js **ä½¿ç”¨ Q-Network é€‰æ‹©ç ç‡**
- **è°ƒè¯•å’Œä¼˜åŒ–è¶…å‚æ•°**ï¼ˆå­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ç­‰ï¼‰
